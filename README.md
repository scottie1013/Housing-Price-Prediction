# Housing-Price-Prediction
## Motivation:
Since I am aiming to become a data/business analyst, I think the best project for me would be something that helps me get familiarize with the data processing/visualization process as well as some ML application. I found out this Kaggle competition on California housing price Prediction(a classic one).
Because it is a well-known dataset and have been practiced by many others, I was be able to get help by referencing others ideas and codes to further guide me to complete this project. I then submit my results and it came out pretty great.
Exploratory Data Analysis:
I first download the datasets from the Kaggle website, load them into google colab, Then I used visualizations to check on some of the key features. I found out that Although there are over 80 features in this dataset, a lot of which has missing values thus I assume it would have less impact on the prediction. Some of the Numerical values have outliers which should also be addressed later.
One interesting finding is that for years the house was being built, I found out that older and newer houses generally have a higher price and not so much for the years in between. It somehow make sense since the newer ones comes with better technology and the older ones have more sentimental/historical values.
## Missing Values
As For Missing values, one question that naturally come up is the what value should we replace the Missing values with, median, mode, 0, NA? In my understanding, It all comes down to what characteristic each feature has. For simplicity(time constraintâ˜º) reasons, I simply replaced missing values with 0s for numerical values and NA for categorical variables. Yet in practice, I assume you have to dig into each feature and understand the relationship between them to come up with a more reasonable replacement
## Standardization:
Most of the features and even the Target variables does not follow a normal distribution. By doing the log transformation on the numerical values, This would ensure that all of the features are on the same scale, which is necessary for accurate predictions. It also helps to reduce the influence of outliers on the model, as well as reducing the computational cost of training the model.
An alternative would be to Normalize the dataset. Considering the fact that most features have a skewed distribution, I decided to go with the standardization.
Another observation that came up during the process is whether I am supposed to transform all data with log transformation. After doing the research, I found out that log-transformation is used when data is right skewed. A better method to use box-cox transformation(which I did not really implement here).
## Machine Learning Time:
Since we are predicting continuous values here, we use Regression Models to tackle this problem. Three models I chose are Ridge, Lasso, and ElasticNet. I first split the train data into train and validation set and run the regression without tunning the parameters and calculated the mean square error as well as training and validation scores. which sets up the baseline of the prediction. Then I used grid search to help me find the best parameters for each model.
After the tunning and fitting, I then input the three different prediction results produced by the models into XGboost to help me form a new results. The reason being that XGboost are able reconstruct the results based on all three models advantages and reduce the prediction error generated by a single model.
In the end I fit the test data into the models I developed and get back the submission results.
## Conclusion/Reflection:
Through this process, I get to learn how to manipulate data and perform feature engineering accordingly. I also gained experience in tunning parameters(here Gridsearch did most of the jobs for me) for machine learning models, although I feel like to truly understand how to tune parameters, I need to dig into the mathematics behind all of this algorithms in order to gain a more holistic understanding.
I also acquired the knowledge on how to optimize the results of the model by coming up with creative solutions (e.g. different ways in handling missing values and their distribution, feeding all three models into XGBoost to get a balanced results)
I have also seen people using TensorFlow to tackle this problem which is also eye- opening so I might try it sometimes

Overall, My biggest take away throughout this process is that there are a lot of new data science knowledge waiting for me to learn in order to become a true data maters

## Below are the final outcome
<img width="814" alt="Screenshot 2022-12-16 at 3 13 19 PM" src="https://user-images.githubusercontent.com/114832226/208181629-e4fe7ea0-23a4-4368-8b70-a4739ac134b5.png">
